import os
import requests
import pandas as pd
import gzip
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import traceback

# ==========================================
# CONFIGURATION
# ==========================================
db_url = os.environ.get("SUPABASE_DATABASE_URL")
if not db_url:
    raise ValueError("Missing SUPABASE_DATABASE_URL environment variable")

engine = create_engine(db_url)

CHAIN_ID = "7290027600007"
CHAIN_NAME = "砖驻专住"
BASE_URL = "http://prices.shufersal.co.il/"

# 专砖转 注拽 拽专转 注 砖!
WATCHLIST_STORES = ["001", "042", "116", "205", "300", "002"]

DATA_DIR = "ETL_Process_Shufersal"
STORES_DIR = os.path.join(DATA_DIR, "stores")
PRICES_DIR = os.path.join(DATA_DIR, "prices")
os.makedirs(STORES_DIR, exist_ok=True)
os.makedirs(PRICES_DIR, exist_ok=True)

# ==========================================
# EMAIL CONFIGURATION
# ==========================================
EMAIL_SENDER = os.environ.get("EMAIL_SENDER") 
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD") 
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")

def send_email_report(subject, body):
    if not all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        print("[WARNING] Email credentials not fully set. Skipping email alert.")
        return

    try:
        msg = MIMEMultipart()
        msg['From'] = EMAIL_SENDER
        msg['To'] = EMAIL_RECEIVER
        msg['Subject'] = subject
        msg.attach(MIMEText(body, 'plain', 'utf-8'))

        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(EMAIL_SENDER, EMAIL_PASSWORD)
        server.send_message(msg)
        server.quit()
        print("[SUCCESS] Email report sent successfully.")
    except Exception as e:
        print(f"[ERROR] Failed to send email: {e}")

# ==========================================
# DATA NORMALIZATION DICTIONARIES
# ==========================================
CITY_MAPPING = {
    '转"': '转 ', '转': '转 ', '转  - 驻': '转 ', '转 -驻': '转 ', '专转  ': '转 ',
    '-': '专砖', '专砖': '专砖', '': '专砖',
    '专砖"爪': '专砖 爪', '专砖爪': '专砖 爪', '专砖': '专砖 爪',
    '专-砖注': '专 砖注', '专砖注': '专 砖注', '"砖': '专 砖注',
    '转-砖砖': '转 砖砖', '专砖-驻': '专砖 驻', ' 专 注拽': '专 注拽',
    '驻转-转拽': '驻转 转拽', '驻转-转拽': '驻转 转拽', '驻转转拽': '驻转 转拽', '驻转 转拽': '驻转 转拽',
    '-专拽': ' 专拽', '驻专-住': '驻专 住', '驻专 住 爪驻': '驻专 住',
    '专转-': '专转 ', '专转-砖专': '专转 砖专', '爪驻-专': '爪驻 专',
    '拽注': '拽注 注转', '拽注': '拽注 注转', '注': '拽专转 注',
    '专注转': '注', '爪专-转': '爪专 转', '转-': '转 ',
    '住-爪': '住 爪', '祝-': '祝 ',
    'NaN': ' 注', 'nan': ' 注'
}

REGION_MAPPING = {
    '驻拽': '专', '专 ': '专', '专 注拽': '砖专', '转': '专', 
    '拽': ' 砖专', '注': '专', '专': ' 砖专', '砖': '专', 
    '砖拽': '专', '专 ': '专', '专 注拽': '专', '专 砖注': '专', 
    '专转 爪拽': '专', '转 砖': '专', '转 砖': '爪驻', '转 砖砖': '专砖 住', 
    '转专 注转': '专砖 住', ' 专拽': '专', ' 专专': '砖专', '': '爪驻', 
    '转 驻专': '砖专', '转 ': '专', '注转 ': '爪驻', '注转 注': '爪驻', 
    '注转 砖': '专', '注转': '专', '专': '专', '': '专', 
    '转  专': '爪驻', ' 砖专': '砖专', '专爪': '砖专', '专 注拽': '爪驻', 
    '专': '爪驻', '': '专', '驻': '爪驻', '爪专 转': '爪驻', 
    '专砖': '爪驻', '专': '爪驻', '': '专', '专': '专', 
    '专转 专': '爪驻', '': '专', '': '专', '拽注 注转': '爪驻', 
    '专': '专', '专砖': '专砖 住', '专': '爪驻', '驻专 专': '爪驻', 
    '驻专 ': '砖专', '驻专 专': '砖专', '驻专 住': '砖专', '驻专 拽专注': '爪驻', 
    '驻专 转专': '爪驻', '专专': '爪驻', '专': '爪驻', ' 注': ' 专', 
    '砖专转 爪': '专砖 住', ' 注拽': '爪驻', '注': '专', 
    '注 注转': ' 砖专', '专转 转': '专', '转专': '专', 
    '注 ': ' 砖专', '注转': '爪驻', '爪驻 专': '专', 
    '砖专 砖专': '砖专', '专': '爪驻', '祝 ': '爪驻', '住 爪': '专', 
    '爪专转': '爪驻', '砖专': '爪驻', '转': '砖专', '住': '专', 
    '住': '爪驻', '注专': '专', '注 砖专': '爪驻', '注': '爪驻', 
    '注驻': '爪驻', '注专': '专', '驻专住 ': '爪驻', '驻专住': '砖专', 
    '驻转 转拽': '专', '爪专 ': '砖专', '爪专 砖': '砖专', '爪专': '砖专', 
    '爪驻转': '爪驻', '拽': '砖专', '拽爪专': '爪驻', '拽专转 ': '专', 
    '拽专转 转': '爪驻', '拽专转 拽': '爪驻', '拽专转 转': '专', 
    '拽专转 ': '爪驻', '拽专转 注': '爪驻', '拽专转 爪拽': '爪驻', 
    '拽专转 住驻专': ' 砖专', '拽专转 砖': '爪驻', '专砖 注': '专', 
    '专砖 驻': '爪驻', '专砖 爪': '专', '专': '专', '专转': '专', 
    '专住': '爪驻', '专': '专', '专转 ': '专', '专转 砖专': '专', 
    '专注': '砖专', '砖专转': '专', '砖': '专', '砖转': '专', 
    '砖驻专注': '爪驻', '转 ': '专', '转 ': '砖专'
}

def normalize_city_name(city_name):
    if not isinstance(city_name, str) or city_name.strip() == '': 
        return ' 注'
    city_name = city_name.strip()
    if city_name in CITY_MAPPING: return CITY_MAPPING[city_name]
    if city_name.startswith('拽专转 '): return city_name.replace('拽专转 ', '拽专转 ')
    return city_name

# ==========================================
# ETL LOGIC (Original Robust Version)
# ==========================================
def get_download_links():
    print("[INFO] Connecting to Shufersal website to fetch links...")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    links = []
    found_targets = set()
    targets_needed = 1 + len(WATCHLIST_STORES)
    
    for page_num in range(1, 251):
        # print(f"[INFO] Scanning page {page_num}...")
        try:
            response = requests.get(f"{BASE_URL}?page={page_num}", headers=headers, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for tr in soup.find_all('tr'):
                a_tag = tr.find('a', href=True)
                if a_tag:
                    row_text = tr.get_text(separator=' ', strip=True)
                    if CHAIN_ID in row_text:
                        for word in row_text.split():
                            if CHAIN_ID in word:
                                is_target = False
                                if f"Stores{CHAIN_ID}" in word and "Stores" not in found_targets:
                                    is_target = True
                                    found_targets.add("Stores")
                                for store in WATCHLIST_STORES:
                                    target = f"PriceFull{CHAIN_ID}-{store}"
                                    if target in word and target not in found_targets:
                                        is_target = True
                                        found_targets.add(target)
                                        break
                                
                                if is_target:
                                    url = a_tag['href']
                                    if url.startswith('/'): url = BASE_URL.rstrip('/') + url
                                    links.append((word, url))
                                    print(f"  [+] Found: {word}")
                                break
            
            if len(found_targets) >= targets_needed: break
        except Exception as e:
            print(f"[ERROR] Failed on page {page_num}: {e}")
            break
            
    return links

def fast_parse_xml(file_path, item_tag):
    items = []
    with gzip.open(file_path, 'rb') as f:
        context = ET.iterparse(f, events=('end',))
        for event, elem in context:
            if elem.tag == item_tag or elem.tag.lower() == item_tag.lower() or elem.tag.endswith(item_tag):
                item_data = {child.tag: child.text for child in elem}
                items.append(item_data)
                elem.clear()
    return pd.DataFrame(items)

def run_full_etl():
    print("======================================")
    print("[START] Starting STREAMING ETL for Shufersal...")
    print("======================================")
    
    start_time = datetime.now()
    stats = {"stores_files": 0, "price_files": 0, "total_prices_inserted": 0}

    with engine.begin() as conn:
        conn.execute(text(f"""
            INSERT INTO "Dim_Chains" (chain_id, chain_name) 
            VALUES ('{CHAIN_ID}', '{CHAIN_NAME}') 
            ON CONFLICT (chain_id) DO NOTHING;
        """))

    all_links = get_download_links()
    
    stores_links = [l for l in all_links if "Stores" in l[0]]
    price_links = [l for l in all_links if "PriceFull" in l[0]]
    
    print(f"[INFO] Found {len(stores_links)} store files and {len(price_links)} price files.")
    stats["stores_files"] = len(stores_links)
    stats["price_files"] = len(price_links)

    # --- 砖 : 拽爪 住驻 注专 (注 专) ---
    for fname, url in stores_links:
        print(f"\n[STEP] Processing Stores: {fname}")
        local_path = os.path.join(STORES_DIR, fname + ".gz")
        resp = requests.get(url)
        with open(local_path, 'wb') as f: f.write(resp.content)
        
        df = fast_parse_xml(local_path, 'STORE')
        df.columns = [c.upper() for c in df.columns]
        df = df.rename(columns={'STOREID': 'StoreId', 'STORENAME': 'StoreName', 'CITY': 'City'})
        
        df['City'] = df['City'].apply(normalize_city_name)
        
        with engine.begin() as conn:
            cities = df[['City']].drop_duplicates().rename(columns={'City': 'city_name'})
            cities['region'] = cities['city_name'].map(lambda x: REGION_MAPPING.get(x, ' 专'))
            
            for idx, row in cities.iterrows():
                conn.execute(text('INSERT INTO "Dim_City" (city_name, region) VALUES (:city_name, :region) ON CONFLICT (city_name) DO UPDATE SET region = EXCLUDED.region'), row.to_dict())
            
            df['store_id'] = CHAIN_ID + "-" + df['StoreId'].astype(str).str.zfill(3)
            df['chain_id'] = CHAIN_ID
            stores_to_db = df[['store_id', 'chain_id', 'StoreName', 'City']].rename(columns={'StoreName': 'store_name', 'City': 'city'})
            
            stores_to_db.to_sql('temp_stores', conn, if_exists='replace', index=False)
            conn.execute(text("""
                INSERT INTO "Dim_Stores" (store_id, chain_id, store_name, city)
                SELECT store_id, chain_id, store_name, city FROM temp_stores
                ON CONFLICT (store_id) DO UPDATE SET store_name = EXCLUDED.store_name, city = EXCLUDED.city;
            """))
            conn.execute(text("DROP TABLE temp_stores;"))
            
        print(f"  [SUCCESS] Dim_Stores and Dim_City updated.")

    # --- 砖 : 拽爪 专 爪专 ---
    for fname, url in price_links:
        print(f"\n[STEP] Processing Prices: {fname}")
        local_path = os.path.join(PRICES_DIR, fname + ".gz")
        resp = requests.get(url)
        with open(local_path, 'wb') as f: f.write(resp.content)
        
        df = fast_parse_xml(local_path, 'Item')
        
        products = df[['ItemCode', 'ItemName', 'ManufacturerName']].drop_duplicates(subset=['ItemCode']).copy()
        products = products.rename(columns={'ItemCode': 'barcode', 'ItemName': 'item_name', 'ManufacturerName': 'manufacturer'})
        products['category'] = ''
        
        prices = df[['ItemCode', 'PriceUpdateDate', 'ItemPrice']].copy()
        prices = prices.rename(columns={'ItemCode': 'barcode', 'PriceUpdateDate': 'sample_date', 'ItemPrice': 'price'})
        prices['chain_id'] = CHAIN_ID
        store_num = fname.split('-')[1].split('_')[0] if '-' in fname else "001"
        prices['store_id'] = f"{CHAIN_ID}-{store_num}"
        prices['sample_date'] = pd.to_datetime(prices['sample_date'])

        print(f"  [DB] Injecting {len(prices)} rows to Supabase...")
        with engine.begin() as conn:
            products.to_sql('temp_products', conn, if_exists='replace', index=False)
            conn.execute(text("""
                INSERT INTO "Dim_Products" (barcode, item_name, category, manufacturer)
                SELECT barcode, item_name, category, manufacturer FROM temp_products
                ON CONFLICT (barcode) DO NOTHING;
            """))
            conn.execute(text("DROP TABLE temp_products;"))
            
            prices.to_sql('Fact_Prices', conn, if_exists='append', index=False, chunksize=1000, method='multi')
        
        stats["total_prices_inserted"] += len(prices)
        print(f"  [SUCCESS] Store {store_num} prices inserted.")

    end_time = datetime.now()
    duration = round((end_time - start_time).total_seconds() / 60, 2)
    
    print("\n======================================")
    print(f"[DONE]  All data processed successfully in {duration} minutes!")
    print("======================================")
    
    report_body = f"""Shufersal Data Pipeline - SUCCESS 

Run Time: {duration} minutes
Store Files Processed: {stats['stores_files']}
Price Files Processed: {stats['price_files']}
Total Price Rows Inserted: {stats['total_prices_inserted']}

Your Supermarket DSS is up to date! 
"""
    send_email_report(" ETL Success: Shufersal", report_body)

if __name__ == "__main__":
    try:
        run_full_etl()
    except Exception as e:
        error_tb = traceback.format_exc()
        print(f"\n[CRITICAL ERROR] Pipeline failed:\n{error_tb}")
        error_body = f"Shufersal Data Pipeline - FAILED \n\nError details:\n{error_tb}"
        send_email_report(" ETL FAILED: Shufersal", error_body)
        raise e