import os
import gzip
import requests
from bs4 import BeautifulSoup
import pandas as pd
from sqlalchemy import create_engine, text
import lxml.etree as ET
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import traceback

# ==========================================
# CONFIGURATION
# ==========================================
db_url = os.environ.get("SUPABASE_DATABASE_URL")
if not db_url:
    raise ValueError("Missing SUPABASE_DATABASE_URL environment variable")

engine = create_engine(db_url)

BASE_URL = "http://prices.shufersal.co.il/"
CHAIN_ID = "7290027600007"
CHAIN_NAME = "砖驻专住"

# 爪专转 转拽转 转   拽转
DATA_DIR = "ETL_Process_Shufersal"
STORES_DIR = os.path.join(DATA_DIR, "stores")
PRICES_DIR = os.path.join(DATA_DIR, "prices")
os.makedirs(STORES_DIR, exist_ok=True)
os.makedirs(PRICES_DIR, exist_ok=True)

# ==========================================
# EMAIL CONFIGURATION
# ==========================================
# 砖转 住 砖专 砖 -GitHub 注专 砖转 
EMAIL_SENDER = os.environ.get("EMAIL_SENDER") 
EMAIL_PASSWORD = os.environ.get("EMAIL_PASSWORD") 
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER")

def send_email_report(subject, body):
    """驻拽爪 砖转  转专 驻专"""
    if not all([EMAIL_SENDER, EMAIL_PASSWORD, EMAIL_RECEIVER]):
        print("[WARNING] Email credentials not fully set. Skipping email alert.")
        return

    try:
        msg = MIMEMultipart()
        msg['From'] = EMAIL_SENDER
        msg['To'] = EMAIL_RECEIVER
        msg['Subject'] = subject

        msg.attach(MIMEText(body, 'plain', 'utf-8'))

        # 转专转 砖专转 Gmail
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(EMAIL_SENDER, EMAIL_PASSWORD)
        server.send_message(msg)
        server.quit()
        print("[SUCCESS] Email report sent successfully.")
    except Exception as e:
        print(f"[ERROR] Failed to send email: {e}")

# ==========================================
# DATA NORMALIZATION DICTIONARIES
# ==========================================
CITY_MAPPING = {
    '转"': '转 ', '转': '转 ', '转  - 驻': '转 ', '转 -驻': '转 ', '专转  ': '转 ',
    '-': '专砖', '专砖': '专砖', '': '专砖',
    '专砖"爪': '专砖 爪', '专砖爪': '专砖 爪', '专砖': '专砖 爪',
    '专-砖注': '专 砖注', '专砖注': '专 砖注', '"砖': '专 砖注',
    '转-砖砖': '转 砖砖', '专砖-驻': '专砖 驻', ' 专 注拽': '专 注拽',
    '驻转-转拽': '驻转 转拽', '驻转-转拽': '驻转 转拽', '驻转转拽': '驻转 转拽', '驻转 转拽': '驻转 转拽',
    '-专拽': ' 专拽', '驻专-住': '驻专 住', '驻专 住 爪驻': '驻专 住',
    '专转-': '专转 ', '专转-砖专': '专转 砖专', '爪驻-专': '爪驻 专',
    '拽注': '拽注 注转', '拽注': '拽注 注转', '注': '拽专转 注',
    '专注转': '注', '爪专-转': '爪专 转', '转-': '转 ',
    '住-爪': '住 爪', '祝-': '祝 ',
    'NaN': ' 注', 'nan': ' 注'
}

REGION_MAPPING = {
    '驻拽': '专', '专 ': '专', '专 注拽': '砖专', '转': '专', 
    '拽': ' 砖专', '注': '专', '专': ' 砖专', '砖': '专', 
    '砖拽': '专', '专 ': '专', '专 注拽': '专', '专 砖注': '专', 
    '专转 爪拽': '专', '转 砖': '专', '转 砖': '爪驻', '转 砖砖': '专砖 住', 
    '转专 注转': '专砖 住', ' 专拽': '专', ' 专专': '砖专', '': '爪驻', 
    '转 驻专': '砖专', '转 ': '专', '注转 ': '爪驻', '注转 注': '爪驻', 
    '注转 砖': '专', '注转': '专', '专': '专', '': '专', 
    '转  专': '爪驻', ' 砖专': '砖专', '专爪': '砖专', '专 注拽': '爪驻', 
    '专': '爪驻', '': '专', '驻': '爪驻', '爪专 转': '爪驻', 
    '专砖': '爪驻', '专': '爪驻', '': '专', '专': '专', 
    '专转 专': '爪驻', '': '专', '': '专', '拽注 注转': '爪驻', 
    '专': '专', '专砖': '专砖 住', '专': '爪驻', '驻专 专': '爪驻', 
    '驻专 ': '砖专', '驻专 专': '砖专', '驻专 住': '砖专', '驻专 拽专注': '爪驻', 
    '驻专 转专': '爪驻', '专专': '爪驻', '专': '爪驻', ' 注': ' 专', 
    '砖专转 爪': '专砖 住', ' 注拽': '爪驻', '注': '专', 
    '注 注转': ' 砖专', '专转 转': '专', '转专': '专', 
    '注 ': ' 砖专', '注转': '爪驻', '爪驻 专': '专', 
    '砖专 砖专': '砖专', '专': '爪驻', '祝 ': '爪驻', '住 爪': '专', 
    '爪专转': '爪驻', '砖专': '爪驻', '转': '砖专', '住': '专', 
    '住': '爪驻', '注专': '专', '注 砖专': '爪驻', '注': '爪驻', 
    '注驻': '爪驻', '注专': '专', '驻专住 ': '爪驻', '驻专住': '砖专', 
    '驻转 转拽': '专', '爪专 ': '砖专', '爪专 砖': '砖专', '爪专': '砖专', 
    '爪驻转': '爪驻', '拽': '砖专', '拽爪专': '爪驻', '拽专转 ': '专', 
    '拽专转 转': '爪驻', '拽专转 拽': '爪驻', '拽专转 转': '专', 
    '拽专转 ': '爪驻', '拽专转 注': '爪驻', '拽专转 爪拽': '爪驻', 
    '拽专转 住驻专': ' 砖专', '拽专转 砖': '爪驻', '专砖 注': '专', 
    '专砖 驻': '爪驻', '专砖 爪': '专', '专': '专', '专转': '专', 
    '专住': '爪驻', '专': '专', '专转 ': '专', '专转 砖专': '专', 
    '专注': '砖专', '砖专转': '专', '砖': '专', '砖转': '专', 
    '砖驻专注': '爪驻', '转 ': '专', '转 ': '砖专'
}

def normalize_city_name(city_name):
    """驻拽爪 砖拽 转 砖 注专  转 驻 """
    if not isinstance(city_name, str) or city_name.strip() == '': 
        return ' 注'
    
    city_name = city_name.strip()
    
    # 专 驻  砖转 转 
    if city_name in CITY_MAPPING: 
        return CITY_MAPPING[city_name]
        
    # 驻 专祝  注专 砖转 '拽专转 ' 驻转 '拽专转 '
    if city_name.startswith('拽专转 '): 
        return city_name.replace('拽专转 ', '拽专转 ')
        
    return city_name

# ==========================================
# ETL LOGIC
# ==========================================
def get_download_links():
    links = []
    print("[INFO] Connecting to Shufersal website to fetch links...")
    session = requests.Session()
    
    for page in range(1, 150):
        #print(f"[INFO] Scanning page {page}...") # 住转专  注 住驻  砖 
        url = f"{BASE_URL}?page={page}"
        resp = session.get(url)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        table = soup.find('table')
        if not table: break
        
        rows = table.find_all('tr')[1:]
        if not rows: break
        
        for row in rows:
            cols = row.find_all('td')
            if len(cols) > 0:
                fname = cols[0].text.strip()
                link = cols[0].find('a')['href']
                
                # 拽 专拽 住驻 专,  专 专 
                if ("Stores" in fname or "PriceFull" in fname) and "Promo" not in fname and "Null" not in fname:
                    print(f"  [+] Found: {fname}")
                    links.append((fname, link))
                    
        if len(links) >= 7:  #  -6 住驻 + 1 拽抓 Stores 拽转 砖
            break
            
    return links

def fast_parse_xml(file_path, tag_name):
    records = []
    with gzip.open(file_path, 'rb') as f:
        context = ET.iterparse(f, events=('end',), tag=tag_name)
        for event, elem in context:
            record = {child.tag: child.text for child in elem}
            records.append(record)
            elem.clear()
            while elem.getprevious() is not None:
                del elem.getparent()[0]
    return pd.DataFrame(records)

def run_full_etl():
    print("======================================")
    print("[START] Starting STREAMING ETL for Shufersal...")
    print("======================================")
    
    start_time = datetime.now()
    stats = {"stores_files": 0, "price_files": 0, "total_prices_inserted": 0}

    # 爪专转 专砖转 住 转   拽转
    with engine.begin() as conn:
        conn.execute(text(f"""
            INSERT INTO "Dim_Chains" (chain_id, chain_name) 
            VALUES ('{CHAIN_ID}', '{CHAIN_NAME}') 
            ON CONFLICT (chain_id) DO NOTHING;
        """))

    all_links = get_download_links()
    
    stores_links = [l for l in all_links if "Stores" in l[0]]
    price_links = [l for l in all_links if "PriceFull" in l[0]]
    
    print(f"[INFO] Found {len(stores_links)} store files and {len(price_links)} price files.")
    stats["stores_files"] = len(stores_links)
    stats["price_files"] = len(price_links)

    # --- 砖 : 拽爪 住驻 注专 ---
    for fname, url in stores_links:
        print(f"\n[STEP] Processing Stores: {fname}")
        local_path = os.path.join(STORES_DIR, fname + ".gz")
        resp = requests.get(url)
        with open(local_path, 'wb') as f: f.write(resp.content)
        
        df = fast_parse_xml(local_path, 'STORE')
        df.columns = [c.upper() for c in df.columns]
        df = df.rename(columns={'STOREID': 'StoreId', 'STORENAME': 'StoreName', 'CITY': 'City'})
        
        # 专 砖转 注专 爪注转 驻拽爪 砖
        df['City'] = df['City'].apply(normalize_city_name)
        
        with engine.begin() as conn:
            # 专拽转 注专 转 (注 驻转)
            cities = df[['City']].drop_duplicates().rename(columns={'City': 'city_name'})
            cities['region'] = cities['city_name'].map(lambda x: REGION_MAPPING.get(x, ' 专'))
            
            for idx, row in cities.iterrows():
                conn.execute(text('INSERT INTO "Dim_City" (city_name, region) VALUES (:city_name, :region) ON CONFLICT (city_name) DO UPDATE SET region = EXCLUDED.region'), row.to_dict())
            
            # 专拽转 住驻
            df['store_id'] = CHAIN_ID + "-" + df['StoreId'].astype(str).str.zfill(3)
            df['chain_id'] = CHAIN_ID
            stores_to_db = df[['store_id', 'chain_id', 'StoreName', 'City']].rename(columns={'StoreName': 'store_name', 'City': 'city'})
            
            # 砖转砖  转 注 专 砖 住驻
            stores_to_db.to_sql('temp_stores', conn, if_exists='replace', index=False)
            conn.execute(text("""
                INSERT INTO "Dim_Stores" (store_id, chain_id, store_name, city)
                SELECT store_id, chain_id, store_name, city FROM temp_stores
                ON CONFLICT (store_id) DO UPDATE SET store_name = EXCLUDED.store_name, city = EXCLUDED.city;
            """))
            conn.execute(text("DROP TABLE temp_stores;"))
            
        print(f"  [SUCCESS] Dim_Stores and Dim_City updated.")

    # --- 砖 : 拽爪 专 爪专 ---
    for fname, url in price_links:
        print(f"\n[STEP] Processing Prices: {fname}")
        local_path = os.path.join(PRICES_DIR, fname + ".gz")
        resp = requests.get(url)
        with open(local_path, 'wb') as f: f.write(resp.content)
        
        df = fast_parse_xml(local_path, 'Item')
        
        products = df[['ItemCode', 'ItemName', 'ManufacturerName']].drop_duplicates(subset=['ItemCode']).copy()
        products = products.rename(columns={'ItemCode': 'barcode', 'ItemName': 'item_name', 'ManufacturerName': 'manufacturer'})
        products['category'] = ''
        
        prices = df[['ItemCode', 'PriceUpdateDate', 'ItemPrice']].copy()
        prices = prices.rename(columns={'ItemCode': 'barcode', 'PriceUpdateDate': 'sample_date', 'ItemPrice': 'price'})
        prices['chain_id'] = CHAIN_ID
        store_num = fname.split('-')[1].split('_')[0] if '-' in fname else "001"
        prices['store_id'] = f"{CHAIN_ID}-{store_num}"
        prices['sample_date'] = pd.to_datetime(prices['sample_date'])

        print(f"  [DB] Injecting {len(prices)} rows to Supabase...")
        with engine.begin() as conn:
            # 专拽 专 砖 爪专 (Bulk)
            products.to_sql('temp_products', conn, if_exists='replace', index=False)
            conn.execute(text("""
                INSERT INTO "Dim_Products" (barcode, item_name, category, manufacturer)
                SELECT barcode, item_name, category, manufacturer FROM temp_products
                ON CONFLICT (barcode) DO NOTHING;
            """))
            conn.execute(text("DROP TABLE temp_products;"))
            
            # 专拽转 专 (Bulk)
            prices.to_sql('Fact_Prices', conn, if_exists='append', index=False, chunksize=1000, method='multi')
        
        stats["total_prices_inserted"] += len(prices)
        print(f"  [SUCCESS] Store {store_num} prices inserted.")

    end_time = datetime.now()
    duration = round((end_time - start_time).total_seconds() / 60, 2)
    
    print("\n======================================")
    print(f"[DONE]  All data processed successfully in {duration} minutes!")
    print("======================================")
    
    # 砖转  爪
    report_body = f"""Shufersal Data Pipeline - SUCCESS 

Run Time: {duration} minutes
Store Files Processed: {stats['stores_files']}
Price Files Processed: {stats['price_files']}
Total Price Rows Inserted: {stats['total_prices_inserted']}

Your Supermarket DSS is up to date! 
"""
    send_email_report(" ETL Success: Shufersal", report_body)

if __name__ == "__main__":
    try:
        run_full_etl()
    except Exception as e:
        error_tb = traceback.format_exc()
        print(f"\n[CRITICAL ERROR] Pipeline failed:\n{error_tb}")
        
        # 砖转  砖
        error_body = f"""Shufersal Data Pipeline - FAILED 

An error occurred during the ETL process.
Error details:
{error_tb}

Please check GitHub Actions logs.
"""
        send_email_report(" ETL FAILED: Shufersal", error_body)
        raise e  # 专拽转 砖   砖 注 砖住拽专驻 砖